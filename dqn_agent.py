# dqn_agent.py
import numpy as np
import random
from collections import deque
import tensorflow as tf
from tensorflow.keras import layers, models, optimizers
import os

class DQNAgent:
    
    def __init__(self, input_shape, action_space, model_path="dqn_model.keras"):
        self.state_shape = input_shape
        self.action_space = action_space
        self.action_size = int(np.prod(action_space.nvec))  # üëà –ø—Ä–∏–≤–µ–ª–∏ –∫ int
        self.memory = deque(maxlen=5000)
        self.gamma = 0.95
        self.epsilon = 1.0
        self.epsilon_min = 0.01
        self.epsilon_decay = 0.995
        self.learning_rate = 0.001
        self.model_path = model_path
        
        if os.path.exists(model_path):
            print("[INFO] –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏–∑ —Ñ–∞–π–ª–∞")
            self.model = tf.keras.models.load_model(model_path, compile=False)
            self.model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=self.learning_rate), loss='mse')
        else:
            print("[INFO] –°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤–æ–π –º–æ–¥–µ–ª–∏")
            self.model = self._build_model()
        
        self.target_model = tf.keras.models.clone_model(self.model)
        self.target_model.set_weights(self.model.get_weights())
    
    def _build_model(self):
        model = tf.keras.Sequential([
            # –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ø–æ‚Äë–∫–∞–¥—Ä–æ–≤–æ
            tf.keras.layers.TimeDistributed(layers.Conv2D(32, (3, 3), activation='relu'),
                                            input_shape=(5, 100, 400, 3)),
            tf.keras.layers.TimeDistributed(layers.MaxPooling2D((2, 2))),
            tf.keras.layers.TimeDistributed(layers.Conv2D(64, (3, 3), activation='relu')),
            tf.keras.layers.TimeDistributed(layers.MaxPooling2D((2, 2))),
            tf.keras.layers.TimeDistributed(layers.Flatten()),
            # —Ç–µ–ø–µ—Ä—å —É–±–∏—Ä–∞–µ–º –æ—Å—å time_steps
            tf.keras.layers.Flatten(),          # ‚Üê (batch, time_steps * features)
            tf.keras.layers.Dense(256, activation='relu'),
            tf.keras.layers.Dense(self.action_size, activation='linear')
        ])
        model.compile(optimizer=optimizers.Adam(self.learning_rate), loss='mse')
        return model

    def update_target_model(self):
        self.target_model.set_weights(self.model.get_weights())

    def remember(self, state, action, reward, next_state, done):
        self.memory.append((state, action, reward, next_state, done))

    def act(self, state):
        if np.random.rand() <= self.epsilon:
            return [np.random.randint(n) for n in self.action_space.nvec]
        q_values = self.model.predict(state[np.newaxis], verbose=0)[0]
        best_index = np.argmax(q_values)
        action = np.unravel_index(best_index, self.action_space.nvec)
        return list(action)

    def replay(self, batch_size):
        if len(self.memory) < batch_size:
            return

        # 1) –≤—ã–±–æ—Ä —Å–ª—É—á–∞–π–Ω–æ–≥–æ –±–∞—Ç—á–∞ –ø–µ—Ä–µ—Ö–æ–¥–æ–≤
        minibatch = random.sample(self.memory, batch_size)
        # —Ä–∞—Å–ø–∞–∫—É–µ–º –≤ —Å–ø–∏—Å–∫–∏
        states, actions, rewards, next_states, dones = zip(*minibatch)

        # 2) –ø—Ä–µ–≤—Ä–∞—Ç–∏–º –≤ –º–∞—Å—Å–∏–≤—ã
        states      = np.stack(states)       # (batch, 5,100,400,3)
        next_states = np.stack(next_states)  # (batch, 5,100,400,3)

        # 3) –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ Q –∏ target-Q —Å—Ä–∞–∑—É –¥–ª—è –≤—Å–µ–≥–æ –±–∞—Ç—á–∞
        q_values         = self.model.predict(states, verbose=0)         # (batch, action_size)
        next_q_model     = self.model.predict(next_states, verbose=0)    # (batch, action_size)
        next_q_target    = self.target_model.predict(next_states, verbose=0)  # (batch, action_size)

        # 4) –ø–æ–¥–≥–æ—Ç–æ–≤–∏–º —Ü–µ–ª–µ–≤—ã–µ –≤–µ–∫—Ç–æ—Ä–∞
        target_q = q_values.copy()  # shape (batch, action_size)

        for i in range(batch_size):
            # –ø–ª–æ—Å–∫–∏–π –∏–Ω–¥–µ–∫—Å –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –¥–µ–π—Å—Ç–≤–∏—è –≤ action_space.nvec
            flat_index = np.ravel_multi_index(actions[i], self.action_space.nvec)

            if dones[i]:
                target_q[i, flat_index] = rewards[i]
            else:
                # –∂–∞–¥–Ω–æ–µ –¥–µ–π—Å—Ç–≤–∏–µ –∏–∑ online‚Äë—Å–µ—Ç–∏
                best_next = np.argmax(next_q_model[i])
                # TD‚Äë–æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø–æ —Ü–µ–ª–µ–≤–æ–π —Å–µ—Ç–∏
                target_q[i, flat_index] = rewards[i] + self.gamma * next_q_target[i, best_next]

        # 5) –æ–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å –Ω–∞ –≤—Å—ë–º –±–∞—Ç—á–µ —Å—Ä–∞–∑—É
        self.model.fit(states, target_q, epochs=1, verbose=0)

        # 6) –∏ –æ–±–Ω–æ–≤–ª—è–µ–º Œµ
        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay
    
    def save(self):
        self.model.save("dqn_model.keras")
        print("[INFO] –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤", self.model_path)

    def load(self):
        self.model = tf.keras.models.load_model(self.model_path)
        print("[INFO] –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏–∑", self.model_path)
